{"cells": [{"metadata": {}, "cell_type": "markdown", "source": "# Coursera Applied Data Science Capstone Project - Week 2 - Restaurants in Zurich"}, {"metadata": {}, "cell_type": "markdown", "source": "Let's start by importing our necessary libraries."}, {"metadata": {"scrolled": true}, "cell_type": "code", "source": "from bs4 import BeautifulSoup\nimport requests\nimport pandas as pd\nimport numpy as np\nimport scipy.stats as stats", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Using BeautifulSoup, we will parse the <a href=\"https://en.wikipedia.org/wiki/List_of_communities_and_neighborhoods_of_San_Diego\">Wikipedia link</a> into html text and extract the information from the <b>Table</b> attribute."}, {"metadata": {}, "cell_type": "code", "source": "#Extract list of neighborhoods from Wikipedia page\nwiki_link = 'https://en.wikipedia.org/wiki/Subdivisions_of_Z\u00fcrich'\nraw_wiki = requests.get(wiki_link).text\n\n#Import and parse the text from Wikipedia using BeautifulSoup\nsoup = BeautifulSoup(raw_wiki,'lxml')", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "#Data of the neighborhoods is inside an html <table> object.\nsoup_table = soup.table\n#Decompose the <div> object inside the table to clean our data\nsoup_table.div.decompose()\nsoup_table", "execution_count": null, "outputs": []}, {"metadata": {"scrolled": true}, "cell_type": "code", "source": "#Loop through the soup object to extract names of neighborhoods\nneighborhoods = []\n\nfor name in soup_table.find_all('a'):\n    neighborhoods.append(str(name.string))\n    \nprint('There are',len(neighborhoods),'neighborhoods in Z\u00fcrich')", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "#import Nominatim to get location data on the neighborhoods\nfrom geopy.geocoders import Nominatim", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "#from geopy.exc import GeocoderTimedOut #Use this if Geocode \n\ndef locationfinder(neighborhoods):\n    geolocator = Nominatim(user_agent='kkha@lab-data.com')\n    locations = []\n    for nbhd in neighborhoods:\n        address = nbhd + ', Canton Z\u00fcrich'\n        location = geolocator.geocode(address)\n        if location is None:\n            pass\n        else:\n            x = [nbhd, location.latitude, location.longitude]\n            locations.append(x)\n    return(locations)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "import pickle\n\n#check if locations pickle created, will load in data if we've already created it before\ntry:\n    with open('locations.pkl', 'rb') as f:\n        locations = pickle.load(f)\n    print('Data loaded.')\nexcept:\n    locations = locationfinder(neighborhoods)\n    \n#with open('locations.pkl', 'wb') as f:\n#    pickle.dump(locations, f)\n\nlocations[0:5]", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Let's create the Dataframe so we can begin to analyze the data.\n\nThrough trial and error, we found that not all neighborhoods corresponded to GPS locations in geocode. This was due to overlapping neighborhoods and smaller sectioning of neighborhoods. It is okay to remove out these neighborhoods because our search radius will be large enough to cover missing areas."}, {"metadata": {}, "cell_type": "code", "source": "df_loc = pd.DataFrame(data=locations)\ndf_loc.columns = ['Neighborhood', 'Latitude', 'Longitude']\n\n#we see we have NaN values, let's drop these.\ndf_loc.dropna(axis=0, inplace=True)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Let's begin to map out the neighborhoods just to visualize where we will be working with."}, {"metadata": {}, "cell_type": "code", "source": "!conda install -c conda-forge folium=0.5.0 --yes\n\nimport folium", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "geolocator = Nominatim(user_agent='kevinle.kha@gmail.com')\nsd_loc = geolocator.geocode('Zurich')\nsd_lat = sd_loc.latitude\nsd_lng = sd_loc.longitude\n\nmap_SD = folium.Map(location=(sd_lat,sd_lng), zoom_start=10)\n\nfor nbhd, lat, lng in zip(df_loc['Neighborhood'], df_loc['Latitude'], df_loc['Longitude']):\n    label = '{}, ZH'.format(nbhd)\n    label = folium.Popup(label)\n    folium.CircleMarker(\n        [lat, lng],\n        radius=5,\n        popup=label,\n        color='blue',\n        fill=False).add_to(map_SD)\n    \nmap_SD", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "You may be wondering why there are no marked neighborhoods between the larger group and smaller group near the border of Mexico. That area is the city of Chula Vista, which does not fall in the jurisdiction of San Diego, therefore is not included in this data.\n\nAwesome! Now that we have the neighborhoods mapped out, let's begin to use our knowledge of Foursquare API to explore restaurant venues from within the neighborhoods. This will help us to fill in data about the types of restaurants and their density within each neighborhood."}, {"metadata": {}, "cell_type": "code", "source": "#Establish Foursquare credentials and version\nCLIENT_ID = 'E1NJSUC205TKLJRJ0LLIGOGXRTPHU5G332HBJA00QLSIXIYP' # your Foursquare ID\nCLIENT_SECRET = 'GF4TL3GCXPIBPUVOWYLG534XI3LYV3OKHXV1ZEW3YZXSAYVD' # your Foursquare Secret\nVERSION = '20190321' # Foursquare API version", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "def getNearbyFoods(names, latitudes, longitudes, radius=4000, LIMIT=100):\n    \n    food_cat = '4d4b7105d754a06374d81259'\n    rest_list=[]\n    #seen = set()\n    for name, lat, lng in zip(names, latitudes, longitudes):\n        print(name)\n            \n        # create the API request URL\n        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}&categoryId={}'.format(\n            CLIENT_ID, \n            CLIENT_SECRET, \n            VERSION, \n            lat, \n            lng, \n            radius,\n            LIMIT,\n            food_cat)\n            \n        # make the GET request\n        results = requests.get(url).json()[\"response\"]['groups'][0]['items']\n         \n        # return only relevant and unique information for each nearby venue\n        for v in results:\n            #if v['venue']['name'] not in seen:\n                #seen.add(v['venue']['name'])\n                rest_list.append([(\n                    name, \n                    lat, \n                    lng, \n                    v['venue']['id'],\n                    v['venue']['name'], \n                    v['venue']['location']['lat'], \n                    v['venue']['location']['lng'],  \n                    v['venue']['categories'][0]['name'])])\n\n    nearby_foods = pd.DataFrame([item for rest_list in rest_list for item in rest_list])\n    nearby_foods.columns = ['Neighborhood', \n                  'Neighborhood Latitude', \n                  'Neighborhood Longitude', \n                  'Venue ID',\n                  'Venue', \n                  'Venue Latitude', \n                  'Venue Longitude', \n                  'Venue Category']\n    \n    return(nearby_foods)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "#check if restaurant data pickle created, will load in data if we've already created it before\ntry:\n    with open('sd_rests.pkl', 'rb') as f:\n        sd_rests = pickle.load(f)\n    print('Data loaded.')\nexcept:\n    sd_rests = getNearbyFoods(names=df_loc['Neighborhood'],\n                            latitudes = df_loc['Latitude'],\n                            longitudes = df_loc['Longitude'])", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "sd_rests.to_pickle('sd_rests.pkl')", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "print('There are {} unique restaurants and {} unique restaurant categories'.format(\n    len(sd_rests['Venue'].unique()),\n    len(sd_rests['Venue Category'].unique())))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "sd_rests.columns = ['Neighborhood', 'Neighborhood Latitude', 'Neighborhood Longitude', 'Venue ID', 'Venue', 'Venue Latitude', 'Venue Longitude', 'Venue Category']", "execution_count": null, "outputs": []}, {"metadata": {"scrolled": false}, "cell_type": "code", "source": "print(sd_rests.shape)\nsd_rests.head()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "We will now begin to manipulate the data by using getdummies and get a onehot table so it will be easier for our machine learning algorithms to classify our data."}, {"metadata": {}, "cell_type": "code", "source": "#one hot encoding\nsd_onehot = pd.get_dummies(sd_rests[['Venue Category']], prefix=\"\", prefix_sep=\"\")\n\n#add neighborhoods column back to dataframe\nsd_onehot['Neighborhood'] = sd_rests['Neighborhood']\n\n#move neighborhood column to first column\nfixed_columns = [sd_onehot.columns[-1]] + list(sd_onehot.columns[:-1])\nsd_onehot = sd_onehot[fixed_columns]\n\n#just to make sure we moved the columns correctly\nsd_onehot.head()", "execution_count": null, "outputs": []}, {"metadata": {"scrolled": true}, "cell_type": "code", "source": "sd_grouped = sd_onehot.groupby('Neighborhood').mean().sort_values('Neighborhood').reset_index()\nsd_grouped", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "from sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "Sum_of_squared_distances = []\nsd_grouped_clustering = sd_grouped.drop('Neighborhood', 1)\n\nK = range(1,15)\nfor k in K:\n    km = KMeans(n_clusters=k)\n    km = km.fit(sd_grouped_clustering)\n    Sum_of_squared_distances.append(km.inertia_)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "plt.plot(K, Sum_of_squared_distances, 'bx-')\nplt.xlabel('k')\nplt.ylabel('Sum_of_squared_distances')\nplt.title('Elbow Method For Optimal k')\nplt.show()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "In our Elbow method, we see that K will continuously decrease so we will  set cluster limit to 4. That is when the changes sum of squared distances becomes negligible as we increase K."}, {"metadata": {}, "cell_type": "code", "source": "#set number of clusters\nkclusters = 10\n\n#run k-means clustering\nkmeans = KMeans(n_clusters=kclusters, random_state=0).fit(sd_grouped_clustering)\n\nkmeans.labels_[0:10]", "execution_count": null, "outputs": []}, {"metadata": {"scrolled": true}, "cell_type": "code", "source": "#Apply the cluster labels to our original location dataframe\nsd_clustered = df_loc.sort_values('Neighborhood').reset_index(drop=True)\nsd_clustered['Cluster Labels'] = kmeans.labels_\n\nsd_clustered.head()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "We are now going to map out the neighborhoods and color them for easier visualization of the grouping."}, {"metadata": {}, "cell_type": "code", "source": "#import plot colors\nimport matplotlib.cm as cm\nimport matplotlib.colors as colors", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# create map\nmap_clusters = folium.Map(location=[sd_lat, sd_lng], zoom_start=10)\n\n# set color scheme for the clusters\nx = np.arange(kclusters)\nys = [i + x + (i*x)**2 for i in range(kclusters)]\ncolors_array = cm.jet(np.linspace(0, 1, len(ys)))\njet = [colors.to_hex(i) for i in colors_array]\n\n# add markers to the map\nmarkers_colors = []\nfor lat, lon, poi, cluster in zip(sd_clustered['Latitude'], sd_clustered['Longitude'], sd_clustered['Neighborhood'], sd_clustered['Cluster Labels']):\n    label = folium.Popup(str(poi) + ' Cluster ' + str(cluster), parse_html=True)\n    folium.CircleMarker(\n        [lat, lon],\n        radius=5,\n        popup=label,\n        color=jet[cluster-1],\n        fill=True,\n        fill_color=jet[cluster-1],\n        fill_opacity=0.7).add_to(map_clusters)\n       \nmap_clusters", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "#Find the count of neighborhoods within each cluster to see how our model worked\nsd_clustered[['Neighborhood','Cluster Labels']].groupby('Cluster Labels').count()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Now that we have our neighborhoods clustered, let's dive into each cluster and do some statistical testing to find out the similarity of the clusters"}, {"metadata": {}, "cell_type": "code", "source": "#Create a new dataframe of only Neighborhood, Venue data, and Cluster Labels\nsd_rests_cat = pd.DataFrame(data=sd_rests[['Neighborhood','Venue ID','Venue','Venue Latitude', 'Venue Longitude', 'Venue Category']])\n\n#Create a dictionary that maps each neighborhood to its assigned cluster from K-Means\ncluster_nbhd = sd_clustered[['Neighborhood', 'Cluster Labels']]\nnbhd_cluster_dict = dict(zip(cluster_nbhd['Neighborhood'], cluster_nbhd['Cluster Labels']))\nnbhd_cluster_dict\n\n#Map the dictionary to our new dataframe\nsd_rests_cat['Cluster Labels'] = sd_rests_cat['Neighborhood'].map(nbhd_cluster_dict)\nsd_rests_cat.head()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "#list to span numbered clusters\nc_list = list(np.arange(0,kclusters,1))\n\n#dictionary of dataframes to separate out restaurants by cluster\nd = {c: pd.DataFrame() for c in c_list}\n\n# d[#] where # means the numbered cluster to make our data more easily understood\nfor c in c_list:\n    d[c] = sd_rests_cat.loc[sd_rests_cat['Cluster Labels'] == c].drop(['Cluster Labels'],1).reset_index(drop=True)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "What wasn't readily understood at the beginning was that our explore call was collecting duplicate restaurants when the search radius overlapped through neighborhoods. This will clean the data to only include unique restaurants and have each restaurant appear only once per cluster."}, {"metadata": {}, "cell_type": "code", "source": "def get_unique_rests(data):\n\n    unique_list = []\n    seen = set()\n\n    for i in range(0,data.shape[0]):\n        if data['Venue ID'][i] not in seen:\n            seen.add(data['Venue ID'][i])\n            unique_list.append(data.iloc[i])\n        \n        df_unique = pd.DataFrame(data=unique_list).reset_index(drop=True)\n    return(df_unique)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "c = {c: pd.DataFrame() for c in c_list}\nfor x in c_list:\n    c[x] = get_unique_rests(d[x]) ", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "c[0].head()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "cf = {x: pd.DataFrame() for x in c_list}\n\nfor x in c_list:\n    cf[x] = c[x][['Venue','Venue Category']].groupby('Venue Category').count()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "from functools import reduce\n\ncfct = reduce(lambda x,y: pd.merge(x,y, on='Venue Category', how='outer'), [cf[x] for x in c_list])", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "string = 'Cluster '\nclusters = list(map(str, range(0, kclusters)))\ncluster_list = [string + c for c in clusters]", "execution_count": null, "outputs": []}, {"metadata": {"scrolled": false}, "cell_type": "code", "source": "cfct.columns = cluster_list\ncfct.fillna(value=0, inplace=True)\ncfct.head()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Using the Student's T-test, we found the p-values between each of the clusters to find the similarity bewteen the clusters. T-testing requires one to understand the null hypothesis that two sample means are the same. If the p-value is closer to 1, we can accept the null hypothesis which means the mean of the clusters are similar. Alternately, if the p-value is closer to 0, we can reject the null hypothesis, which means that the mean of the two clusters are NOT similar (they are different).\n"}, {"metadata": {}, "cell_type": "code", "source": "p = []\npl = {}\n\nfor i in c_list:\n    for j in c_list:\n        p.append(stats.ttest_ind(cf[i],cf[j]).pvalue[0])\n    pl[i] = p\n    p=[]\n    \npl_df = pd.DataFrame(pl)\npl_df", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Comparing a cluster to itself will be 1 (since the means are the same if you compare the same set), so we will drop those values by making them NULL or NaN."}, {"metadata": {}, "cell_type": "code", "source": "for i in c_list:\n    pl_df[i][i] = np.NaN\n    \npl_df", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# idxmax() will find the index of the maximum of that column/row\nmax = pl_df.idxmax()\n# idxmin() will find the index of the minimum of that column/row\nmin = pl_df.idxmin()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "for i in c_list:\n    print('For Cluster',i,'the most similar Cluster is',max[i])", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "for i in c_list:\n    print('For Cluster',i,'the most dissimilar Cluster is',min[i])", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Methodlogy\n\nIn this project, we will begin by extracting a list of all the neighborhoods within San Diego. Wikipedia has a list we can use, so we will parse out the raw HTML text of the website for our benefit using BeautifulSoup. Once the neighborhoods are extracted from the text, we will use Geocoder and the names of the neighborhood to find their latitude and longitude. \n\nOnce we have the locations of each neighborhood, we will begin to work with our Foursquare API. "}, {"metadata": {}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.7", "language": "python"}, "language_info": {"name": "python", "version": "3.7.9", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 2}